"""
ç®€åŒ–çš„æ°´å°æ£€æµ‹ç³»ç»Ÿ - åŸºäºSOLIDåŸåˆ™é‡æ„
ä¿æŒ make_prompt_for_detection å’Œ make_prompt_for_watermark_generation ä¸å˜
"""
import os
import sys
import argparse
import json
import random
from tqdm import tqdm

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
sys.path.insert(0, project_root)

# å¯¼å…¥é‡æ„åçš„æ¨¡å—
from utils.base_utils import load_pkl, save_pkl, string2token_nums
from utils.api_service import generation_by_qwen
from utils.prompt_generator import (
    make_prompt_for_watermark_generation, 
    make_prompt_for_detection,
    convert_to_detector_examples
)
from utils.case_manager import WatermarkCase, CaseManager
from utils.metrics import compute_metrics
from utils.config import QUALITY_CONFIG

# è®¾ç½®éšæœºç§å­
random.seed(42)

class WatermarkDetectionSystem:
    """æ°´å°æ£€æµ‹ç³»ç»Ÿä¸»ç±»"""
    
    def __init__(self, use_confidence: bool = False, max_epochs: int = 2, max_examples: int = 200):
        self.use_confidence = use_confidence
        self.max_epochs = max_epochs
        self.max_examples = max_examples
        self.case_manager = CaseManager(max_examples)
        self.performance_log = []
    
    def process_single_text(self, original_text: str) -> WatermarkCase:
        """å¤„ç†å•ä¸ªæ–‡æœ¬"""
        # è·å–å½“å‰ç¤ºä¾‹
        examples_dict = self.case_manager.get_examples_dict()
        
        # ç”Ÿæˆæ°´å°æ–‡æœ¬
        prompt_gen = make_prompt_for_watermark_generation(
            original_text, examples_dict, max_examples=10
        )
        orig_tokens = string2token_nums(original_text)
        watermarked_text = generation_by_qwen(prompt_gen, orig_tokens)
        
        # åˆ›å»ºæ¡ˆä¾‹
        case = WatermarkCase(original_text, watermarked_text)
        
        # æ›´æ–°æ ‡ç­¾å’Œç½®ä¿¡åº¦
        detector_examples = convert_to_detector_examples(examples_dict)
        case.update_labels(detector_examples, self.use_confidence)
        
        return case
    
    def train_epoch(self, original_texts: list, epoch: int) -> dict:
        """è®­ç»ƒä¸€ä¸ªepoch"""
        print(f"\n=== ç¬¬ {epoch+1}/{self.max_epochs} è½®è®­ç»ƒ ===")
        
        labels, preds = [], []
        epoch_metrics = {"success": 0, "failure": 0}
        
        pbar = tqdm(original_texts, desc=f"ç¬¬{epoch+1}è½®")
        
        for idx, original_text in enumerate(pbar):
            # å¤„ç†æ–‡æœ¬
            case = self.process_single_text(original_text)
            
            # æ·»åŠ åˆ°æ¡ˆä¾‹ç®¡ç†å™¨
            self.case_manager.add_case(case)
            
            # è®°å½•æŒ‡æ ‡
            if case.label == 'Good':
                labels.append("1")
                preds.append("1")
                epoch_metrics["success"] += 1
            else:
                labels.append("1")
                preds.append("0")
                epoch_metrics["failure"] += 1
            
            # å®šæœŸæµ‹è¯•åŸå§‹æ–‡æœ¬æ£€æµ‹
            if idx % 5 == 0 and len(self.case_manager.success_cases) > 0:
                self._test_original_detection(original_texts, labels, preds, idx)
            
            # æ›´æ–°è¿›åº¦æ¡
            if len(labels) > 0:
                acc = sum([1 for l, p in zip(labels, preds) if l == p]) / len(labels)
                high_q = len(self.case_manager.get_high_quality_cases())
                pbar.set_postfix({'acc': f"{acc:.2%}", 'high_q': high_q})
            
            # ä¿å­˜æ£€æŸ¥ç‚¹
            if (idx + 1) % 50 == 0:
                self._save_checkpoint(epoch)
        
        pbar.close()
        return self._calculate_epoch_metrics(labels, preds, epoch_metrics, len(original_texts))
    
    def _test_original_detection(self, original_texts: list, labels: list, preds: list, current_idx: int):
        """æµ‹è¯•åŸå§‹æ–‡æœ¬æ£€æµ‹"""
        # éšæœºé€‰æ‹©ä¸€ä¸ªä¸åŒçš„æ–‡æœ¬
        test_idx = random.randint(0, len(original_texts)-1)
        while test_idx == current_idx:
            test_idx = random.randint(0, len(original_texts)-1)
        
        test_original = original_texts[test_idx]
        detector_examples = convert_to_detector_examples(self.case_manager.get_examples_dict())
        
        if self.use_confidence:
            from utils.confidence_calculators import get_confidence_voting
            confidence = get_confidence_voting(test_original, detector_examples, rounds=3)
            predicted_original = confidence < 0.5
        else:
            from utils.api_service import call_model_api
            prompt = make_prompt_for_detection(test_original, detector_examples)
            response = call_model_api(prompt).lower()
            predicted_original = 'original' in response
        
        # è®°å½•ç»“æœ
        labels.append("0")
        preds.append("0" if predicted_original else "1")
    
    def _calculate_epoch_metrics(self, labels: list, preds: list, epoch_metrics: dict, total_samples: int) -> dict:
        """è®¡ç®—epochæŒ‡æ ‡"""
        human_rec, machine_rec, avg_rec, acc, precision, recall, f1 = compute_metrics(labels, preds)
        
        metrics = {
            "epoch": len(self.performance_log) + 1,
            "accuracy": f"{acc:.2%}",
            "f1": f"{f1:.2%}",
            "watermarked_recall": f"{machine_rec/100:.2%}",
            "original_recall": f"{human_rec/100:.2%}",
            "success_rate": f"{epoch_metrics['success']/total_samples:.2%}",
            "high_quality_cases": len(self.case_manager.get_high_quality_cases()),
        }
        
        self.performance_log.append(metrics)
        
        print(f"\næœ¬è½®æŒ‡æ ‡:")
        print(f"å‡†ç¡®ç‡: {metrics['accuracy']}")
        print(f"F1åˆ†æ•°: {metrics['f1']}")
        print(f"æ°´å°å¬å›ç‡: {metrics['watermarked_recall']}")
        print(f"åŸå§‹å¬å›ç‡: {metrics['original_recall']}")
        print(f"é«˜è´¨é‡æ¡ˆä¾‹æ•°: {metrics['high_quality_cases']}")
        
        return metrics
    
    def _save_checkpoint(self, epoch: int):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        checkpoint_path = f"../results/checkpoint_{epoch}.pkl"
        save_pkl(self.case_manager.get_examples_dict(), checkpoint_path)
    
    def train(self, original_texts: list):
        """è®­ç»ƒä¸»å¾ªç¯"""
        for epoch in range(self.max_epochs):
            self.train_epoch(original_texts, epoch)
        
        return self._generate_final_report()
    
    def _generate_final_report(self) -> dict:
        """ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š"""
        quality_scores = [case.quality_score for case in self.case_manager.success_cases]
        confidence_levels = [case.confidence for case in self.case_manager.success_cases]
        
        report = {
            "performance_history": self.performance_log,
            "final_stats": {
                "success_cases": len(self.case_manager.success_cases),
                "failure_cases": len(self.case_manager.failure_cases),
                "high_quality_ratio": len(self.case_manager.get_high_quality_cases()) / max(1, len(self.case_manager.success_cases)),
                "avg_quality_score": sum(quality_scores) / len(quality_scores) if quality_scores else 0,
                "avg_confidence": sum(confidence_levels) / len(confidence_levels) if confidence_levels else 0,
            },
            "config_used": {
                "use_confidence": self.use_confidence,
                "max_epochs": self.max_epochs,
                "quality_threshold": QUALITY_CONFIG['threshold']
            }
        }
        
        return report


def main():
    parser = argparse.ArgumentParser(description='ç®€åŒ–çš„æ°´å°æ£€æµ‹ç³»ç»Ÿ')
    parser.add_argument('--data_dir', type=str, default='../../data/',
                       help='æ•°æ®é›†ç›®å½•è·¯å¾„')
    parser.add_argument('--output_dir', type=str, default='../results/',
                       help='ç»“æœè¾“å‡ºç›®å½•')
    parser.add_argument('--use_confidence', action='store_true',
                       help='æ˜¯å¦ä½¿ç”¨ç½®ä¿¡åº¦ç³»ç»Ÿ')
    parser.add_argument('--max_epochs', type=int, default=2,
                       help='æœ€å¤§è®­ç»ƒè½®æ•°')
    parser.add_argument('--debug', action='store_true',
                       help='è°ƒè¯•æ¨¡å¼')
    
    args = parser.parse_args()
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    if not os.path.exists(args.output_dir):
        os.makedirs(args.output_dir)
    
    # æ ¹æ®è°ƒè¯•æ¨¡å¼è°ƒæ•´å‚æ•°
    if args.debug:
        max_examples = 20
        print("ğŸ› è°ƒè¯•æ¨¡å¼å·²å¯ç”¨")
    else:
        max_examples = 200
    
    # åŠ è½½æ•°æ®
    dataset_path = os.path.join(args.data_dir, "common", "train", "train_humans.pkl")
    dataset_path = os.path.normpath(dataset_path)
    
    if not os.path.exists(dataset_path):
        print(f"âš ï¸ æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {dataset_path}")
        print("ğŸ¯ ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®è¿›è¡Œæµ‹è¯•...")
        original_texts = [
            f"This is a sample human-written text {i}. It contains natural language patterns."
            for i in range(10 if args.debug else 100)
        ]
    else:
        original_texts = load_pkl(dataset_path)
        limit = 20 if args.debug else 1000
        original_texts = original_texts[:limit]
    
    print(f"åŠ è½½äº† {len(original_texts)} ç¯‡åŸå§‹æ–‡ç« ")
    
    # åˆ›å»ºå¹¶è¿è¡Œç³»ç»Ÿ
    system = WatermarkDetectionSystem(
        use_confidence=args.use_confidence,
        max_epochs=args.max_epochs,
        max_examples=max_examples
    )
    
    final_report = system.train(original_texts)
    
    # ä¿å­˜æŠ¥å‘Š
    with open(os.path.join(args.output_dir, "final_report_simple.json"), "w") as f:
        json.dump(final_report, f, indent=2)
    
    print(f"\n=== æœ€ç»ˆç»Ÿè®¡ ===")
    stats = final_report["final_stats"]
    print(f"æˆåŠŸæ¡ˆä¾‹æ•°: {stats['success_cases']}")
    print(f"å¤±è´¥æ¡ˆä¾‹æ•°: {stats['failure_cases']}")
    print(f"é«˜è´¨é‡æ¡ˆä¾‹æ¯”ä¾‹: {stats['high_quality_ratio']:.2%}")
    print(f"å¹³å‡è´¨é‡åˆ†æ•°: {stats['avg_quality_score']:.2%}")
    print(f"ä½¿ç”¨ç½®ä¿¡åº¦: {'æ˜¯' if args.use_confidence else 'å¦'}")


if __name__ == '__main__':
    main()
