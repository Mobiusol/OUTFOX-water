"""
OUTFOXæ£€æµ‹ç³»ç»Ÿ - ç®€åŒ–é‡æ„ç‰ˆ
ç§»é™¤å¤æ‚çš„ç½®ä¿¡åº¦ç³»ç»Ÿå’Œå†—ä½™é€»è¾‘ï¼Œä¿æŒæ ¸å¿ƒåŠŸèƒ½
"""

import random
random.seed(42)
import os
import sys
import argparse
from tqdm import tqdm
import json

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
sys.path.insert(0, project_root)

from utils.utils import (
    load_pkl, save_pkl, call_model_api, compute_metrics,
    get_comprehensive_similarity, string2token_nums
)

# é‡è¦ï¼šä¿æŒè¿™ä¸¤ä¸ªå‡½æ•°ä¸å˜
from utils.utils import (
    make_prompt_for_watermark_generation,
    generation_by_qwen,
    make_prompt_for_detection,
    convert_to_detector_examples
)

# ç®€åŒ–é…ç½®
CONFIG = {
    'max_epochs': 2,
    'max_examples': 100,
    'max_prompt_examples': 10,
    'similarity_threshold': 0.7,
    'use_simple_evaluation': True
}

print("ğŸ¯ ç®€åŒ–æ¨¡å¼å¯ç”¨ - ç§»é™¤å¤æ‚çš„ç½®ä¿¡åº¦å’Œè´¨é‡è¯„åˆ†ç³»ç»Ÿ")


class SimpleCase:
    """ç®€åŒ–çš„æ¡ˆä¾‹ç±» - ç§»é™¤å¤æ‚é€»è¾‘"""
    
    def __init__(self, original, watermarked):
        self.original = original
        self.watermarked = watermarked
        self.label = 'Unknown'
        self.is_good = False
    
    def evaluate(self, detector_examples):
        """ç®€åŒ–çš„è¯„ä¼°é€»è¾‘"""
        try:
            # ç®€å•æ£€æµ‹æ°´å°æ–‡æœ¬
            watermark_prompt = make_prompt_for_detection(self.watermarked, detector_examples[:5])
            watermark_response = call_model_api(watermark_prompt).lower()
            
            # ç®€å•æ£€æµ‹åŸå§‹æ–‡æœ¬
            original_prompt = make_prompt_for_detection(self.original, detector_examples[:5])
            original_response = call_model_api(original_prompt).lower()
            
            # ç®€å•åˆ¤æ–­é€»è¾‘
            watermark_detected = 'watermarked' in watermark_response
            original_correct = 'original' in original_response
            
            self.is_good = watermark_detected and original_correct
            self.label = 'Good' if self.is_good else 'Bad'
            
        except Exception as e:
            print(f"è¯„ä¼°å¤±è´¥: {e}")
            self.is_good = False
            self.label = 'Bad'
    
    def to_dict(self):
        """è½¬æ¢ä¸ºå­—å…¸æ ¼å¼"""
        return {
            'original': self.original,
            'watermarked': self.watermarked,
            'label': self.label
        }


class SimplifiedTrainer:
    """ç®€åŒ–çš„è®­ç»ƒå™¨ - ç§»é™¤å¤æ‚çš„ç½®ä¿¡åº¦å’Œè´¨é‡è¯„åˆ†"""
    
    def __init__(self):
        self.examples = {"success": [], "failure": []}
        self.performance_log = []
    
    def process_text(self, original_text):
        """å¤„ç†å•ä¸ªæ–‡æœ¬ - ç®€åŒ–é€»è¾‘"""
        # ç”Ÿæˆæ°´å°æ–‡æœ¬
        prompt = make_prompt_for_watermark_generation(
            original_text, 
            self.examples, 
            max_examples=CONFIG['max_prompt_examples']
        )
        
        orig_tokens = string2token_nums(original_text)
        watermarked_text = generation_by_qwen(prompt, orig_tokens)
        
        # åˆ›å»ºæ¡ˆä¾‹
        case = SimpleCase(original_text, watermarked_text)
        
        # è¯„ä¼°æ¡ˆä¾‹
        detector_examples = convert_to_detector_examples(self.examples)
        case.evaluate(detector_examples)
        
        # æ·»åŠ åˆ°é€‚å½“çš„åˆ—è¡¨
        if case.is_good:
            self.examples["success"].append(case.to_dict())
            # ä¿æŒæ•°é‡é™åˆ¶
            if len(self.examples["success"]) > CONFIG['max_examples']:
                self.examples["success"] = self.examples["success"][-CONFIG['max_examples']:]
        else:
            self.examples["failure"].append(case.to_dict())
        
        return case
    
    def run_epoch(self, texts, epoch):
        """è¿è¡Œå•ä¸ªepoch - ç®€åŒ–é€»è¾‘"""
        labels, predictions = [], []
        
        pbar = tqdm(enumerate(texts), total=len(texts), desc=f"ç¬¬{epoch+1}è½®")
        
        for idx, original_text in pbar:
            try:
                # å¤„ç†æ–‡æœ¬
                case = self.process_text(original_text)
                
                # è®°å½•ç»“æœ
                labels.append("1")  # æ°´å°æ–‡æœ¬
                predictions.append("1" if case.is_good else "0")
                
                # æ¯5ä¸ªæ ·æœ¬æµ‹è¯•ä¸€ä¸ªåŸå§‹æ–‡æœ¬
                if idx % 5 == 0 and len(texts) > 1:
                    test_idx = random.randint(0, len(texts)-1)
                    while test_idx == idx:
                        test_idx = random.randint(0, len(texts)-1)
                    
                    test_original = texts[test_idx]
                    test_case = SimpleCase(test_original, test_original)
                    detector_examples = convert_to_detector_examples(self.examples)
                    test_case.evaluate(detector_examples)
                    
                    labels.append("0")  # åŸå§‹æ–‡æœ¬
                    predictions.append("0" if 'original' in call_model_api(
                        make_prompt_for_detection(test_original, detector_examples[:5])
                    ).lower() else "1")
                
                # æ›´æ–°è¿›åº¦æ¡
                if len(labels) > 0:
                    acc = sum(1 for l, p in zip(labels, predictions) if l == p) / len(labels)
                    pbar.set_postfix({
                        'acc': f"{acc:.2%}",
                        'good': len(self.examples["success"]),
                        'bad': len(self.examples["failure"])
                    })
                
            except Exception as e:
                print(f"å¤„ç†æ–‡æœ¬ {idx} æ—¶å‡ºé”™: {e}")
                labels.append("1")
                predictions.append("0")
        
        pbar.close()
        
        # è®¡ç®—æŒ‡æ ‡
        if labels and predictions:
            human_rec, machine_rec, avg_rec, acc, precision, recall, f1 = compute_metrics(labels, predictions)
            
            metrics = {
                "epoch": epoch + 1,
                "accuracy": f"{acc:.2%}",
                "f1": f"{f1:.2%}", 
                "watermarked_recall": f"{machine_rec:.1f}%",
                "original_recall": f"{human_rec:.1f}%",
                "success_cases": len(self.examples["success"]),
                "failure_cases": len(self.examples["failure"])
            }
            
            self.performance_log.append(metrics)
            return metrics
        
        return {}


def main():
    """ä¸»å‡½æ•° - ç®€åŒ–é€»è¾‘"""
    parser = argparse.ArgumentParser(description='ç®€åŒ–çš„OUTFOXæ£€æµ‹ç³»ç»Ÿ')
    parser.add_argument('--data_dir', type=str, default='../../data/',
                       help='æ•°æ®é›†ç›®å½•è·¯å¾„')
    parser.add_argument('--output_dir', type=str, default='../results/',
                       help='ç»“æœè¾“å‡ºç›®å½•')
    parser.add_argument('--debug', action='store_true',
                       help='è°ƒè¯•æ¨¡å¼')
    
    args = parser.parse_args()
    
    # è°ƒè¯•æ¨¡å¼é…ç½®
    if args.debug:
        CONFIG['max_epochs'] = 1
        CONFIG['max_examples'] = 20
        print("ğŸ› è°ƒè¯•æ¨¡å¼å¯ç”¨")
    
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    os.makedirs(args.output_dir, exist_ok=True)
    
    # åˆå§‹åŒ–è®­ç»ƒå™¨
    trainer = SimplifiedTrainer()
    
    # åŠ è½½æ•°æ®
    dataset_path = os.path.join(args.data_dir, "common", "train", "train_humans.pkl")
    dataset_path = os.path.normpath(dataset_path)
    
    if not os.path.exists(dataset_path):
        print(f"âš ï¸  æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {dataset_path}")
        print("ğŸ¯ ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®...")
        texts = [f"Sample text {i} for testing watermark detection." for i in range(50)]
    else:
        texts = load_pkl(dataset_path)[:500]  # é™åˆ¶æ•°é‡
    
    print(f"ğŸ“š åŠ è½½äº† {len(texts)} ç¯‡æ–‡ç« ")
    
    # è¿è¡Œè®­ç»ƒ
    for epoch in range(CONFIG['max_epochs']):
        print(f"\n=== ç¬¬ {epoch+1}/{CONFIG['max_epochs']} è½®è®­ç»ƒ ===")
        
        metrics = trainer.run_epoch(texts, epoch)
        
        if metrics:
            print(f"å‡†ç¡®ç‡: {metrics['accuracy']}")
            print(f"F1åˆ†æ•°: {metrics['f1']}")
            print(f"æ°´å°å¬å›ç‡: {metrics['watermarked_recall']}")
            print(f"åŸå§‹å¬å›ç‡: {metrics['original_recall']}")
            print(f"æˆåŠŸæ¡ˆä¾‹: {metrics['success_cases']}")
        
        # ä¿å­˜æ£€æŸ¥ç‚¹
        checkpoint_path = os.path.join(args.output_dir, f"checkpoint_simplified_{epoch}.pkl")
        save_pkl(trainer.examples, checkpoint_path)
    
    # ä¿å­˜æœ€ç»ˆç»“æœ
    final_report = {
        "performance_history": trainer.performance_log,
        "final_examples": trainer.examples,
        "config_used": CONFIG
    }
    
    report_path = os.path.join(args.output_dir, "final_report_simplified.json")
    with open(report_path, "w", encoding='utf-8') as f:
        json.dump(final_report, f, indent=2, ensure_ascii=False)
    
    print(f"\n=== æœ€ç»ˆç»Ÿè®¡ ===")
    print(f"æˆåŠŸæ¡ˆä¾‹æ•°: {len(trainer.examples['success'])}")
    print(f"å¤±è´¥æ¡ˆä¾‹æ•°: {len(trainer.examples['failure'])}")
    print(f"ğŸ“„ æŠ¥å‘Šå·²ä¿å­˜: {report_path}")
    print("\nğŸ‰ è®­ç»ƒå®Œæˆï¼")


if __name__ == '__main__':
    main()
